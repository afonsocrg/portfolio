{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed4cb0b",
   "metadata": {
    "id": "0ed4cb0b"
   },
   "source": [
    "# Deep Learning (IST, 2021-22)\n",
    "## Homework 1\n",
    "\n",
    "### Question 1\n",
    "\n",
    "> In this exercise, you will show that the binary and multinomial logistic losses are convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d5ee0",
   "metadata": {},
   "source": [
    "Before solving these exercises, we want to recall some derivative rules that will be helpful throughout this document:\n",
    "\n",
    "**Reciprocal Rule**:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} \\frac{1}{f(x)} = \\frac{-f'(x)}{f(x)^{2}}\n",
    "$$\n",
    "\n",
    "\n",
    "**Quotient Rule**:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x}\\frac{f(x)}{g(x)} = \n",
    "\\frac{f'(x)g(x) - f(x)g'(x)}{g(x)^{2}}\n",
    "$$\n",
    "\n",
    "**Chain Rule**:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x}f(g(x)) = \n",
    "f'(g(x)) \\cdot g'(x)\n",
    "$$\n",
    "\n",
    "When applied to the $log$ function, we have that\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x}log(g(x)) = \n",
    "\\frac{1}{g(x)} \\cdot g'(x) = \\frac{g'(x)}{g(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9cadba",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1.1 (5 points)\n",
    "> The sigmoid activation function is $\\sigma(z) = 1 / (1 +e^{−z})$, where $z \\in \\mathbb{R}$.  \n",
    "> 1. Show that its derivative can be expressed as $\\sigma′(z) = \\sigma(z)(1 − \\sigma(z))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Aue9QN7y73R2",
   "metadata": {
    "id": "Aue9QN7y73R2"
   },
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  \\sigma'(z) {} & = \\left( \\frac{1}{1 + e^{-z}} \\right)'\n",
    "  = \\frac{-(1+e^{-z})'}{(1 + e^{-z})^{2}}\n",
    "  = \\frac{e^{-z}}{(1 + e^{-z})^{2}}\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac{1 + e^{-z} - 1}{(1 + e^{-z})^{2}}\n",
    "  = \\frac{1 + e^{-z}}{(1 + e^{-z})^{2}} - \\frac{1}{(1 + e^{-z})^{2}}\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac{1}{1 + e^{-z}} - \\frac{1}{1 + e^{-z}} \\cdot \\frac{1}{1 + e^{-z}}\n",
    "  \\\\ \\\\ &\n",
    "  = \\sigma(z) - \\sigma(z)\\cdot \\sigma(z)\n",
    "  \\\\ \\\\ &\n",
    "  = \\sigma(z)(1-\\sigma(z))\n",
    "  \\\\ \\\\ &\n",
    "  Q.E.D.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0051081d",
   "metadata": {
    "id": "0051081d"
   },
   "source": [
    "---\n",
    "#### 1.2 (5 points)\n",
    "\n",
    "> Consider a binary classification problem with $y \\in {\\pm 1}$.  \n",
    "> A binary logistic regression model defines $P(y = +1 | x;w,b) = \\sigma(z)$ with $z = w^{⊺}\\phi(x) + b$.  \n",
    "> The binary logistic loss is\n",
    "$$\n",
    "L(z; y) = \\left\\{ \\begin{array}{ll}\n",
    "-log (\\sigma(z)) & \\mbox{if} & y = +1 \\\\\n",
    "-log(1-\\sigma(z)) & \\mbox{if} & y = -1\n",
    "\\end{array}\\right.\n",
    "= -\\frac{1 + y}{2}log\\sigma(z) - \\frac{1-y}{w}log(1-\\sigma(z))\n",
    "$$  \n",
    "> where $y$ is the gold label. Assume $y = +1$ and  \n",
    "> 1. compute the first and second derivatives of $L'(z; y = +1)$ and $L''(z; y = +1)$ w.r.t. $z$.  \n",
    "> 2. Show that the binary logistic loss is convex as a function of $z$.  \n",
    "> *Hint: Use the result from the previous question.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K1jDfzhaEVTz",
   "metadata": {
    "id": "K1jDfzhaEVTz"
   },
   "source": [
    "We can derive\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  (-log(\\sigma(z)))' {} & = -\\frac{1}{\\sigma(z)}\\sigma'(z) \\text{, from the Chain Rule (4)}\n",
    "  \\\\ \\\\ &\n",
    "  = -\\frac{1}{\\sigma(z)}\\sigma(z)(1-\\sigma(z))\n",
    "  \\\\ \\\\ &\n",
    "  = \\sigma(z) - 1\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The second derivative is trivially calculated with:\n",
    "$$\n",
    "(−log(\\sigma(z)))'' = (\\sigma(z) - 1)' = \\sigma'(z) = \\sigma(z)(1-\\sigma(z))\n",
    "$$\n",
    "\n",
    "To demonstrate that the binary logistic loss is convex as a function of $z$, we can recall the following theorem from [Convexity and differentiable functions](https://math.ucr.edu/~res/math133/convex-functions.pdf):\n",
    "\n",
    "> **Theorem 2**: Let $K \\subset \\mathbb{R}$ be an interval, and let $f$ be a real valued function on $K$ with a continuous second derivative. If $f''$ is nonnegative everywhere, then $f$ is convex on $K$.\n",
    "\n",
    "We know that:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  {} & 0 < \\sigma(z) < 1, \\forall z \\in \\mathbb{R}\n",
    "  \\\\ \\\\ &\n",
    "  \\Leftrightarrow -1 < -\\sigma(z) < 0, \\forall z \\in \\mathbb{R}\n",
    "  \\\\ \\\\ &\n",
    "  \\Leftrightarrow 0 < 1 -\\sigma(z) < 1, \\forall z \\in \\mathbb{R}\n",
    "  \\\\ \\\\ &\n",
    "  \\Leftrightarrow 0 < \\sigma(z)(1 -\\sigma(z)) < 1, \\forall z \\in \\mathbb{R}\n",
    "  \\\\ \\\\ &\n",
    "  \\Leftrightarrow 0 < (-log(\\sigma(z)))'' < 1, \\forall z \\in \\mathbb{R} \\\\ \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Hence, we conclude that the second derivative of $-log(\\sigma(z))$ is strictly positive and consequently prove that the binary logistic loss is convex as a function of $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0952c8",
   "metadata": {
    "id": "5d0952c8"
   },
   "source": [
    "---\n",
    "#### 1.3 (5 points)\n",
    "> Let us now turn to the multi-class case, where $y \\in \\{1, ...,K\\}$ with $K \\geq 2$. Let $z \\in \\mathbb{R}^{K}$. The softmax transformation is a function from $\\mathbb{R}^{K}$ to $\\mathbb{R}^{K}$ defined as:\n",
    "$$\n",
    "[softmax(z)]_{j} = \\frac{exp(z_{j})}{\\sum_{k=1}^{K}exp(z_{k})}\n",
    "$$\n",
    "> for $j = 1,...,K$ (the entries of $z$ are called logits or scores).  \n",
    "> 1. Compute the Jacobian matrix of the softmax transformation on point $z$ - This is the $K$-by-$K$ matrix whose $(j,k)$-th entry is $\\frac{\\partial[softmax(x)]_{j}}{\\partial z_{k}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a12c21",
   "metadata": {},
   "source": [
    "We know that the Jacobian Matrix is \"the matrix of all the function's first-order partial derivatives\" ([wikipedia](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)), hence we can derive the following:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  J_{jk} {} & = \\frac{\\partial [softmax(z)]_{j}}{\\partial z_{k}}\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac{\\partial}{\\partial z_{k}} \\frac{exp(z_{j})}{\\sum_{i=1}^{K} exp(z_{i})}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "If $j \\neq k$, we have that\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  \\frac{\\partial}{\\partial z_{k}} \\frac{exp(z_{j})}{\\sum_{i=1}^{K} exp(z_{i})} {} & = exp(z_{j}) \\times \\frac{\\partial}{\\partial z_{k}}\\frac{1}{\\sum_{i=1}^{K} exp(z_{i})}\n",
    "  \\\\ \\\\ &\n",
    "  = exp(z_{j}) \\times \\frac{-\\frac{\\partial}{\\partial z_{k}}\\left(\\sum_{i=1}^{K} exp(z_{i})\\right)}{\\left[\\sum_{i=1}^{K} exp(z_{i})\\right]^{2}} \\text{, from the Reciprocal Rule (1)}\n",
    "  \\\\ \\\\ &\n",
    "  = exp(z_{j}) \\times \\frac{-\\frac{\\partial}{\\partial z_{k}}exp(z_{k})}{\\left[\\sum_{i=1}^{K} exp(z_{i})\\right]^{2}}\n",
    "  \\\\ \\\\ &\n",
    "  = -\\frac{exp(z_{j})exp(z_{k})}{\\left[\\sum_{i=1}^{K} exp(z_{i})\\right]^{2}}\n",
    "  \\\\ \\\\ &\n",
    "  = -\\frac{exp(z_{j})}{\\sum_{i=1}^{K} exp(z_{i})}\\frac{exp(z_{k})}{\\sum_{i=1}^{K} exp(z_{i})}\n",
    "  \\\\ \\\\ &\n",
    "  = -[softmax(z)]_{j} [softmax(z)]_{k}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "If $j = k$, we have that\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  \\frac{\\partial}{\\partial z_{j}} \\frac{exp(z_{j})}{\\sum_{i=1}^{K} exp(z_{i})} {} & = \\frac\n",
    "\t{\\left( \\frac{\\partial}{\\partial z_{j}} exp(z_{j}) \\right) \\left(\\sum_{i=1}^{K} exp(z_{i}) \\right) - exp(z_{j}) \\frac{\\partial}{\\partial z_{j}} \\left(\\sum_{i=1}^{K} exp(z_{i}) \\right)}\n",
    "\t{\\left(\\sum_{i=1}^{K} exp(z_{i}) \\right)^{2}} \\text{, from the Quotient Rule (2)}\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac\n",
    "\t{exp(z_{j})\\left(\\sum_{i=1}^{K} exp(z_{i}) \\right) - exp(z_j)exp(z_j)}\n",
    "\t{\\left(\\sum_{i=1}^{K} exp(z_{i}) \\right)^{2}}\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac\n",
    "\t{exp(z_{j}) \\left(\\left(\\sum_{i=1}^{K} exp(z_{i}) \\right) - exp(z_j) \\right)}\n",
    "\t{\\left(\\sum_{i=1}^{K} exp(z_{i}) \\right) ^{2}}\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac{exp(z_{j})}{\\sum_{i=1}^{K} exp(z_{i})} \\times\n",
    "  \\frac{\\left(\\sum_{i=1}^{K} exp(z_{i}) \\right) - exp(z_j)}{\\sum_{i=1}^{K} exp(z_{i})}\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac{exp(z_{j})}{\\sum_{i=1}^{K} exp(z_{i})}\\left(1-\n",
    "  \\frac{exp(z_j)}{\\sum_{i=1}^{K} exp(z_{i})}\\right)\n",
    "  \\\\ \\\\ &\n",
    "  = [softmax(z)]_{j} (1 - [softmax(z)]_{j})\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We can conclude that the Jacobian matrix is a symmetric matrix with each entry $J_{jk}$ defined as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  J_{jk} {} & = \\frac{\\partial [softmax(z)]_{j}}{\\partial z_{k}}\n",
    "  \\\\ \\\\ &\n",
    "  = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "\t    - [softmax(z)]_{j} [softmax(z)]_{k}, & j \\neq k\\\\\n",
    "\t    [softmax(z)]_{j} (1 - [softmax(z)]_{j}), & j = k\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548bcbd",
   "metadata": {
    "id": "0548bcbd"
   },
   "source": [
    "---\n",
    "#### 1.4 (5 points)\n",
    "> The multinomial logistic loss is defined as $L(z;y=j) = -log[softmax(z)]_{j}$.  \n",
    "> 1. Compute the gradient and Hessian of this loss with respect to $z$ and  \n",
    "> 2. show that this loss is convex with respect to $z$.  \n",
    "> *Hint: Use again the result from the previous question*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1a51b",
   "metadata": {},
   "source": [
    "**Gradient calculation**:  \n",
    "\n",
    "The gradient of the given loss function, with respect to $z$, is given by:\n",
    "\n",
    "$$\n",
    "\\newcommand{\\softmax}[1]{ [softmax(z)]_{#1} }\n",
    "\\newcommand{\\sumezi}{\\left(\\sum_{i=1}^{K} exp(z_{i}) \\right)}\n",
    "\\newcommand{\\pz}[1]{ \\frac{\\partial}{\\partial z_{#1}}}\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  \\nabla L(z; y=j) {} & = \\left[\\begin{matrix}\n",
    "\t\t\t\\pz{1}L(z; y=j) & \\dots & \\pz{K}L(z;y=j)\n",
    "\t\t\\end{matrix}\\right]\n",
    "  \\\\ \\\\ &\n",
    "  = \\left[\\begin{matrix}\n",
    "\t\t\t\\pz{1}-log\\softmax{j}\n",
    "\t\t\t& \\dots\n",
    "\t\t\t& \\pz{K}-log\\softmax{j}\n",
    "\t\t\\end{matrix}\\right]\n",
    "  \\\\ \\\\ &\n",
    "  = \\left[\\begin{matrix}\n",
    "\t\t-\\frac{\\pz{1}\\softmax{j}}{\\softmax{j}}\n",
    "\t\t& \\dots\n",
    "\t\t& -\\frac{\\pz{K}\\softmax{j}}{\\softmax{j}}\n",
    "\t\t\\end{matrix}\\right] \\text{, from the Chain Rule (4)}\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac{-1}{\\softmax{j}}\\left[\\begin{matrix}\n",
    "\t\t\\pz{1}\\softmax{j}\n",
    "\t\t& \\dots\n",
    "\t\t& \\pz{K}\\softmax{j}\n",
    "\t\t\\end{matrix}\\right]\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "From the previous exercise, we can conclude that\n",
    "\n",
    "$$\n",
    "\\newcommand{\\softmax}[1]{ [softmax(z)]_{#1} }\n",
    "\\newcommand{\\sumezi}{\\left(\\sum_{i=1}^{K} exp(z_{i}) \\right)}\n",
    "\\newcommand{\\pz}[1]{ \\frac{\\partial}{\\partial z_{#1}}}\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  J_{k} {} & = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "\t    \\frac{-1}{\\softmax{j}} \\cdot -[softmax(z)]_{j} [softmax(z)]_{k}, & k \\neq j\n",
    "      \\\\\n",
    "\t    \\frac{-1}{\\softmax{j}} \\cdot [softmax(z)]_{j} (1 - [softmax(z)]_{j}), & k = j\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "  \\\\ \\\\ &\n",
    "  = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "\t    [softmax(z)]_{k}, & k \\neq j\n",
    "      \\\\\n",
    "\t    [softmax(z)]_{j} - 1, & k = j\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "**Hessian calculation**:  \n",
    "\n",
    "The Hessian of the Loss function is given as $H = \\nabla^{2}L(z;y=j)$ and each entry of the Hessian is:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  H_{ab} {} & = \\pz{a} \\left( \\nabla L(z;y=j)\\right)_{b}\n",
    "  \\\\ \\\\ &\n",
    "  = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "\t    \\pz{a}[softmax(z)]_{b}, & b \\neq j\n",
    "      \\\\\n",
    "\t    \\pz{a}\\left([softmax(z)]_{j} - 1\\right), & b = j\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "  \\\\ \\\\ &\n",
    "  = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "\t    \\pz{a}[softmax(z)]_{b}, & b \\neq j\n",
    "      \\\\\n",
    "\t    \\pz{a}[softmax(z)]_{j}, & b = j\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "  \\\\ \\\\ &\n",
    "  = \\pz{a}[softmax(z)]_{b}\n",
    "  \\\\ \\\\ &\n",
    "  = \\left\\{\\begin{array}{lll}\n",
    "\t\t\t\t-\\softmax{a}\\softmax{b} & , & a \\neq b \\\\\n",
    "\t\t\t\t\\softmax{a}(1 - \\softmax{a})& , & a = b\n",
    "\t\t\t\t\\end{array}\n",
    "\t\t\t\\right.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "**Convexity proof**:\n",
    "\n",
    "We know that\n",
    "\n",
    " 1. a function $f$ is convex if and only if it has a Positive Semi-Definite Hessian.\n",
    " 2. A symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ is Positive Semi-Definite if, for any $X \\in \\mathbb{R}^{n}, X^{T}AX \\geq 0$.\n",
    "\n",
    "From the previous exercise, we know that the Hessian of the loss function is symmetric, due to the Commutative property of the multiplication ($-\\softmax{a}\\softmax{b} = -\\softmax{b}\\softmax{a}$).\n",
    "Moreover, we have that\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  X^{T}HX {} & = \\left( \\sum_{a}\\softmax{a}\n",
    "\t\t\t\t\t(1-\\softmax{a})x_{a}^{2}\\right) \\\\\n",
    "\t\t\t\t\t&\\quad -\\sum_{a \\neq b}\n",
    "\t\t\t\t\t\t\\softmax{a}\\softmax{b}x_{a}x_{b}\n",
    "  \\\\ \\\\ &\n",
    "  = \\left( \\sum_{a}\\softmax{a}x_{a}^{2}\n",
    "\t\t\t\t\t- \\softmax{a}^{2}x_{a}^{2}\\right) \\\\\n",
    "\t\t\t\t\t&\\quad-\\left[\n",
    "\t\t\t\t\t\t\\left( \n",
    "\t\t\t\t\t\t\t\\sum_{a}\\softmax{a}x_{a}\n",
    "\t\t\t\t\t\t\\right)\n",
    "\t\t\t\t\t\t\\left(\n",
    "\t\t\t\t\t\t\t\\sum_{a}\\softmax{a}x_{a}\n",
    "\t\t\t\t\t\t\\right) \\right.\n",
    "\t\t\t\t\t\t\\left. - \\sum_{a}\n",
    "\t\t\t\t\t\t\t\t\t\\softmax{a}^{2}x_{a}^{2}\n",
    "\t\t\t\t\t\t\t\t\\vphantom{\\left(\\sum\\right)}\n",
    "\t\t\t\t\t\t \\right]\n",
    "  \\\\ \\\\ &\n",
    "  = \\sum_{a}\\softmax{a}x_{a}^{2} - \\sum_{a}\\softmax{a}^{2}x_{a}^{2} \\\\\n",
    "\t\t\t\t\t&\\quad\n",
    "            -\\left(\\sum_{a}\\softmax{a}x_{a}\\right)^{2}\n",
    "\t\t\t\t\t\t\\left. + \\sum_{a}\n",
    "\t\t\t\t\t\t\t\t\t\\softmax{a}^{2}x_{a}^{2}\n",
    "\t\t\t\t\t\t\t\t\\vphantom{\\left(\\sum\\right)}\n",
    "\t\t\t\t\t\t \\right.\n",
    "  \\\\ \\\\ &\n",
    "  = \\sum_{a}\\softmax{a}x_{a}^2 -\n",
    "\t\t\t\t\t\\left( \\sum_{a}\\softmax{a}x_{a}\\right)^2\n",
    "  \\\\ \\\\ &\n",
    "  = \\left(\\sum_{a}\\softmax{a}x_{a}^2\\right)\n",
    "\t\t\t\t\t\\left(\\sum_{a}\\softmax{a}\\right) -\n",
    "\t\t\t\t\t\\left(\\sum_{a}\\softmax{a}x_{a}\\right)^2\n",
    "  \\\\ \\\\ &\n",
    "  = \\left(\\sum_{a}\\left(\\sqrt{\\softmax{a}}\\cdot x_{a}\\right)^2\\right)\n",
    "\t\t\t\t\t\\left(\\sum_{a}\\sqrt{\\softmax{a}}^2\\right) - \\\\\n",
    "\t\t\t\t\t&\\quad \\left( \\sum_{a}\n",
    "\t\t\t\t\t\t\\sqrt{\\softmax{a}}\\cdot\n",
    "\t\t\t\t\t\t\\sqrt{\\softmax{a}}\\cdot x_{a}\\right)^2\n",
    "  \\\\ \\\\ &\n",
    "  = \\left(\\sum_{a}\\left(\\sqrt{\\softmax{a}}\\cdot x_{a}\\right)^2\\right)\n",
    "\t\t\t\t\t\\left(\\sum_{a}\\sqrt{\\softmax{a}}^2\\right) - \\\\\n",
    "\t\t\t\t\t&\\quad \\left( \\sum_{a}\n",
    "\t\t\t\t\t\t\\sqrt{\\softmax{a}}\\cdot x_{a}\\cdot\n",
    "\t\t\t\t\t\t\\sqrt{\\softmax{a}}\\right)^2\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "From Cauchy-Schwarz, we derive\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  {} & \\qquad \\lvert \\langle \\vec{u},\\vec{v} \\rangle \\rvert \\leq \\lVert \\vec{u} \\rVert \\cdot \\lVert \\vec{v} \\rVert\n",
    "  \\\\ \\\\ &\n",
    "  \\Leftrightarrow \\bigg\\lvert \\sum_i u_i v_i \\bigg\\rvert \\leq \\sqrt{\\sum_i u_i^2} \\cdot \\sqrt{\\sum_i v_i^2}\n",
    "  \\\\ \\\\ &\n",
    "  \\Leftrightarrow \\bigg\\lvert\n",
    "\t\t\t\t\\sum_i u_i v_i \\bigg\\rvert^2 \\leq\n",
    "\t\t\t\\sqrt{\\sum_i u_i^2}^2 \\cdot\n",
    "\t\t\t\t\\sqrt{\\sum_i v_i^2}^2\n",
    "  \\\\ \\\\ &\n",
    "  \\Leftrightarrow \\left(\n",
    "\t\t\t\t\\sum_i u_i v_i \\right)^2 \\leq\n",
    "\t\t\t\\left( \\sum_i u_i^2 \\right)\n",
    "\t\t\t\t\\left( \\sum_i v_i^2 \\right)\n",
    "  \\\\ \\\\ &\n",
    "  \\Leftrightarrow 0 \\leq\n",
    "\t\t\t\\left( \\sum_i u_i^2 \\right)\n",
    "\t\t\t\t\\left( \\sum_i v_i^2 \\right) - \\left(\n",
    "\t\t\t\t\\sum_i u_i v_i \\right)^2\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Therefore, we conclude that\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  X^{T}HX {} & = \\left(\\sum_{a}\\left(\\sqrt{\\softmax{a}}\\cdot x_{a}\\right)^2\\right)\n",
    "\t\t\t\t\t\\left(\\sum_{a}\\sqrt{\\softmax{a}}^2\\right) - \\\\\n",
    "\t\t\t\t\t&\\quad \\left( \\sum_{a}\n",
    "\t\t\t\t\t\t\\sqrt{\\softmax{a}}\\cdot x_{a}\\cdot\n",
    "\t\t\t\t\t\t\\sqrt{\\softmax{a}}\\right)^2\n",
    "  \\\\ \\\\ &\n",
    "  \\geq 0 \\quad \\text{by Cauchy-Schwarz}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "*i.e.*, H is Positive Semi-Definite.\n",
    "\n",
    "From 1), we prove that $L(z;y=j)$ is convex with respect to $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b7e4a",
   "metadata": {
    "id": "182b7e4a"
   },
   "source": [
    "---\n",
    "#### 1.5 (5 points)\n",
    "> 1. Show that in a linear model where $z = W\\phi(x) + b$, the multinomial logistic loss is also convex with respect to the model parameters $(W,b)$, and therefore a local minimum is also a global minimum.  \n",
    "> 2. Is this also true in general when z is not a linear function of the model parameters?\n",
    "> *Hint: use the fact that the composition of an affine map $g ∶ \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ with a convex function $f ∶ \\mathbb{R}^{m} \\rightarrow \\mathbb{R}$ is convex, i.e., if $g(u) = Au + c$ and $f$ is convex in $\\mathbb{R}^{m}$, then $(f \\circ g)(u) = f(g(u)) = f(Au + c)$ is convex in $\\mathbb{R}^{m}$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PM2HC7M97y7V",
   "metadata": {
    "id": "PM2HC7M97y7V"
   },
   "source": [
    "**Part 1**:\n",
    "\n",
    "We know that the multinomial logistic loss is $L : \\mathbb{R}^{m} \\rightarrow \\mathbb{R}$ and, from the previous exercise, we know that $L$ is convex w.r.t. $z$. Moreover, we know that $z$ is an affine function of $W$ and $b$: $z = W\\phi(x) + b$.\n",
    "\n",
    "We can express $L(W, b)$ as a the composition of an affine map $z : \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ with a convex function $L : \\mathbb{R}^{m} \\rightarrow \\mathbb{R}$: $L(W, b) = L(z(W, b)) = L(W\\phi(x) + b)$.\n",
    "From the hint given above, we can conclude that the multinomial logistic loss is convex with respect to $(W, b)$.\n",
    "\n",
    "**Part 2**:\n",
    "\n",
    "From [Convexity Theory and Gradient Methods - Angelia Nedić](https://www.ima.umn.edu/materials/2013-2014/ND5.27-6.13.14/20872/IMAConvexity2014-p2.pdf) and [Convex Optimization — Boyd & Vandenberghe](https://web.stanford.edu/class/ee364a/lectures/functions.pdf), we know that\n",
    "\n",
    "> The composition of two functions $f(x) = h(g(x)) = h(g_{1}(x), g_{2}(x), ... , g_{p}(x))$, $h: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}$ and $g: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{p}$ is convex if and only if\n",
    "> 1. each $g_i$ is convex, $h$ is convex and nondecreasing in each argument\n",
    "> 2. each $g_i$ is concave, $h$ is convex and nonincreasing in each argument\n",
    "\n",
    "When applied to this case, the multinomial logistic loss function is denoted by $h$, and $z(W, b)$ is denoted by $g$.\n",
    "\n",
    "We cannot generalize that the multinomial logistic loss is convex with respect to $W$ and $b$: If $z(W, b)$ is not convex w.r.t. $(W, b)$, then the composition will not be convex w.r.t. $(W, b)$ and consequently a local minimum will not necessarily be a global minimum.\n",
    "\n",
    "For example, if $z(W, b) = sin(\\lVert W \\rVert) \\cdot b$, $z$ is not convex w.r.t. $(W, b)$, thus the loss function will not be either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db97fd9",
   "metadata": {
    "id": "5db97fd9"
   },
   "source": [
    "---\n",
    "### Question 2\n",
    "\n",
    "**Regression of house prices with linear models and neural networks**.\n",
    "\n",
    "#### 2.1 (5 points)\n",
    "> Consider the squared error loss\n",
    "$$\n",
    "L(z;y) = \\frac{1}{2}\\|z-y\\|^{2}_{2} = \\frac{1}{2}(z-y)^{T}(z-y)\n",
    "$$\n",
    "> where $z = W^{T}\\phi(x) + b$.  \n",
    "> 1. Show that $L$ is convex with respect to $(W,b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DC2kqikY710A",
   "metadata": {
    "id": "DC2kqikY710A"
   },
   "source": [
    "First, let's prove that $L$ is convez w.r.t $z$:\n",
    "\n",
    "The first derivative of $L$ w.r.t. to $z$ is:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  \\nabla_{z} L(z; y) {} & = \\nabla_{z}\\frac{1}{2}\\|z-y\\|^{2}_{2}\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac{1}{2} \\nabla_{z}\\|z-y\\|^{2}_{2}\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac{1}{2} \\nabla_{z}(z^{T}z - 2y^{T}z + \\|y\\|^{2})\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac{2z - 2y^{T}}{2}\n",
    "  \\\\ \\\\ &\n",
    "  = z - y^{T}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Thus, the second derivative is:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  \\nabla^{2}_{z} L(z; y) {} & = \\nabla_{z}(z - y^{T})\n",
    "  \\\\ \\\\ &\n",
    "  = I\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Since the second derivative of $L$ w.r.t. $z$ is strictly positive, we know that $L$ is convex w.r.t. $z$.\n",
    "\n",
    "We also know that, by definition, $z$ is an affine function of $(W, b)$: $z(W, b) = W^{T}\\phi(x) + b$.\n",
    "\n",
    "Thus, from the hint given in question 1.5, we conclude that $L(z; y)$ is the composition of an affine function $z$ with a convex function $L(z; y)$, thus we prove that $L(W, b) = L(z(W, b))$ is convex w.r.t $(W, b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024b598",
   "metadata": {
    "id": "6024b598"
   },
   "source": [
    "---\n",
    "#### 2.2 - **Ames housing dataset.**\n",
    "\n",
    "> In this question, you will use the Ames housing dataset.  \n",
    "> The dataset lists a large number of properties in the city of Ames, Iowa. Each property is described by a large number of features and has an associated `price tag`. Your goal is to build a predictor for the price of a property given the features describing it.\n",
    "\n",
    "> The Ames housing dataset is similar to other existing datasets (such as the Boston or California datasets), but featuring much richer features. It is also more complex to handle: it contains missing data and both numerical and categorical features. For this task we have prepared the dataset beforehand, filling in missing values and encoding the categorical features using the “one-hot” encoding scheme. The data is included in the file `ames.npz`.  \n",
    "You will train a linear regression model to predict the price of the different properties using the features already in the provided data.\n",
    "\n",
    "> **Please do not use any machine learning library such as scikit-learn or similar for this exercise; just plain linear algebra (the numpy library is fine)**.\n",
    "\n",
    "> **Skeleton code** - For this question, you are recommended (but not required) to use the skeleton scripts `hw1-q2.py` and `utils.py`, and the environment environment.yml, included in the attached zip file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B1n9vDSh_cNW",
   "metadata": {
    "id": "B1n9vDSh_cNW"
   },
   "source": [
    "#### 2.2.a (10 points)\n",
    "> 1. Implement the `update_weights` method of the LinearRegression class in `hw1-q2.py`.  \n",
    "2. Train the model for $150$ epochs using stochastic gradient descent with a learning rate of $0.001$.\n",
    "3. Report the performance on the training and test set.\n",
    "4. Plot the accuracy on both sets as a function of the epoch number.\n",
    "5. Explain the observed difference between the performance on the training and test sets as the number of epochs increases.\n",
    "6. Also report the distance between your weight vector and the weight vector computed analytically as a function of the epoch number (you will need to compute the analytic solution yourself by implementing the `solve_analytically` function).\n",
    "You can run the code with the command\n",
    "> ```\n",
    "$ python hw1-q2.py linear_regression\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b877d8e",
   "metadata": {},
   "source": [
    "In this exercise, we are training Linear Regression model to predict the price of a property, given the features describing it. We will use the Stochastic Gradient Descent to approximate the model parameters to the minimizer of the loss function:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "W^{k+1} & {} = W^{k} - \\eta_{k} \\nabla_{W}(L(W^{k};(x_{t},y_{t})))\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The loss function we aim to minimize is given in the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f7e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(self, X, y):\n",
    "    \"\"\"\n",
    "    return the mean squared error between the model's predictions for X\n",
    "    and the ground truth y values\n",
    "    \"\"\"\n",
    "    yhat = self.predict(X) # phi(X)W\n",
    "    error = yhat - y\n",
    "    squared_error = np.dot(error, error) # sum(error_i * error_i) -> squared error norm\n",
    "    mean_squared_error = squared_error / y.shape[0]\n",
    "    return np.sqrt(mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e23728",
   "metadata": {},
   "source": [
    "Hence we can describe it as:\n",
    "\n",
    "$$\n",
    "L(W, \\phi(X), Y) = \\sqrt{\\frac{\\| \\phi(X)W - Y \\|^2}{N}}\n",
    "$$\n",
    "\n",
    "And its derivative will be:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  \\nabla_W L(W, \\phi(X), Y) {} & = \\nabla_W \\sqrt{\\frac{\\| W\\phi(X) - Y \\|^2}{N}}\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac{1}{\\sqrt{N}} \\nabla_W \\sqrt{\\| W\\phi(X) - Y \\|^2}\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac{\\nabla_W \\| W\\phi(X) - Y \\|^2}{2\\sqrt{N} \\times \\sqrt{\\| W\\phi(X) - Y \\|^2}}\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac{\\nabla_W (W^2 \\phi(X)^2 - 2W\\phi(X)Y + Y^2)}{2\\sqrt{N \\times \\| W\\phi(X) - Y \\|^2}}\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac{2\\phi(X)^T\\phi(X)W - 2\\phi(X)^TY}{2\\sqrt{N \\times \\| W\\phi(X) - Y \\|^2}}\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac{\\phi(X)^T\\phi(X)W - \\phi(X)^TY}{\\sqrt{N \\times \\| W\\phi(X) - Y \\|^2}}\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac{\\phi(X)^T(\\phi(X)W - Y)}{\\sqrt{N \\times \\| W\\phi(X) - Y \\|^2}}\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac{\\phi(X)^T(\\hat{Y} - Y)}{\\sqrt{N \\times \\| \\hat{Y} - Y \\|^2}}  \\quad\\text(N = 1)\n",
    "  \\\\ \\\\ &\n",
    "  = \\frac{\\phi(X)^T(\\hat{Y} - Y)}{\\| \\hat{Y} - Y \\|}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "After training this model for 150 epochs, we got the following results:\n",
    "\n",
    "![Loss graph](results/q2/2_a/linreg_v3/loss.png)\n",
    "\n",
    "The first graph depicts the loss in each epoch, which indicates how far the model predictions were from the real values.\n",
    "Hence, lower losses mean that the model is more accurate.\n",
    "\n",
    "We can see that in the beginning of the training, the loss had a value of 0.521 and 0.727 for the train and test sets, respectively, which rapidly decreased to values between 0.2 and 0.3 in the first 10 epochs.\n",
    "After this moment, the training loss kept decreasing, reaching a minimum loss of 0.111 on epoch 150.\n",
    "However, the test loss started increasing, having values from 0.195 at epoch 71 to 0.239, at epoch 148, and finishing the experiment with a value of 0.238.\n",
    "\n",
    "We can explain this drift between the test and train loss functions because **the model is overfitting the training set**, i.e. the model is adjusting its weights based only on the gradients obtained from the training set.\n",
    "One can use regularization to diminish this problem.\n",
    "\n",
    "We can also note that the model gets some loss spikes during the experiments.\n",
    "We hypothesise this result comes from the randomness of the stochastic gradient descent.\n",
    "Stochastic optimization algorithms may, sometimes, update the model parameters in the wrong \"direction\", leading to slight increases in the attained loss.\n",
    "\n",
    "Below, we have the plot of the distance from the analytical solution in each epoch:\n",
    "![Dist graph](results/q2/2_a/linreg_v3/dist.png)\n",
    "\n",
    "The analytical solution corresponds to the model parameters (weights) that minimize the loss function for the training data set.\n",
    "This value is analytically calculated by a closed form expression.\n",
    "We can see the analytical solution as the target parameters we want our model to learn, and the distance between that solution and the model parameters as \"how far are we from the optimized weights\".\n",
    "\n",
    "The initial parameters start with a distance of 2.879 and monotonically decreases in further epochs, reaching a final distance of 1.452.\n",
    "After experimenting training the model for 1000 epochs, we could see that the distance kept reducing, reaching a minimum 0.669.\n",
    "\n",
    "From this result, we can infer that the model is correctly learning the parameters that best predict the prices of the desired properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MB29XQvOAXGZ",
   "metadata": {
    "id": "MB29XQvOAXGZ"
   },
   "source": [
    "---\n",
    "#### 2.2.b (10 points)\n",
    "\n",
    "> You will now train a neural network model for the same regression problem. **Without using any neural network toolkit**,\n",
    "> 1. Implement a feed-forward network with a single hidden layer to solve the regression problem above, including the gradient back-propagation algorithm which is needed to train the model. You can achieve this by implementing the `__init__()`, `update_weight`, and `predict` methods of the provided `NeuralRegression` class.\n",
    "2. Use $150$ hidden units, a `ReLU` activation function for the hidden layer, and a mean squared error loss in the output.\n",
    "3. Train the model with stochastic gradient descent with a learning rate of $0.001$ for $150$ epochs.\n",
    "4. Initialize biases with zero vectors and values in weight matrices with $w_{ij} \\thicksim \\mathcal{N}(\\mu,\\sigma^{2})$ with $\\mu = 0.1$ and $\\sigma^{2} = 0.1^2$ (hint: use `numpy.random.normal`).\n",
    "5. Report the performance on the training and test set.\n",
    "6. Plot the accuracy in both sets as a function of the epoch number.\n",
    "7. Compare the observed difference between the performance in the training and test sets with that observed in the linear model.\n",
    "You can do this with the command\n",
    "> ```\n",
    "$ python hw1-q2.py nn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ktp1qot4NRu",
   "metadata": {
    "id": "4ktp1qot4NRu"
   },
   "source": [
    "Our single layer feedforward network is given by the following expression:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "f(x) = z^{(2)}(x) &= o(W^{(2)}\\cdot h(x) + b^{(2)})\n",
    "      = W^{(2)}\\cdot h(x) + b^{(2)} \\\\\n",
    "     &= W^{(2)}\\cdot g(z^{(1)}(x)) + b^{(2)} \\\\\n",
    "     &= W^{(2)}\\cdot g(W^{(1)}\\cdot x + b^{(1)}) + b^{(2)}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "Where:\n",
    "- $o(x) = x$. This applies for regression networks.\n",
    "- $g(x) = ReLU(x)$, which is our activation function.\n",
    "\n",
    "Throughout the training of the model, we are tasked with reducing the loss given by:\n",
    "$$\n",
    "L(f(x), y) = \\frac{1}{2}\\|f(x)-y\\|^{2} = \\frac{1}{2}\\|z^{(2)}(x)-y\\|^{2}\n",
    "$$\n",
    "By the gradient rules, we have that:\n",
    "$$\n",
    "\\nabla_{f(x)}L(f(x), y) = f(x) - y \\\\\n",
    "\\nabla_{z^{(2)}}L(f(x), y) = z^{(2)} - y\n",
    "$$\n",
    "\n",
    "For the hidden layer gradients, we have the following expressions:\n",
    "$$\n",
    "\\nabla_{h^{(l)}}L(f(x),y) = W^{(l+1)^{T}}\\nabla_{z^{(l+1)}}L(f(x),y) \\\\\n",
    "\\nabla_{z^{(l)}}L(f(x),y) =\n",
    "\\nabla_{h^{(l)}}L(f(x),y)\\odot g'(z^{(l)}) \\\\\n",
    "g'(x) = 1_{x>0} \\\\\n",
    "\\nabla_{W^{(l)}L(f(x),y)} =\n",
    "\\nabla_{z^{(l)}}L(f(x),y)h^{(l-1)^{T}} \\\\\n",
    "\\nabla_{b^{(l)}}L(f(x),y) =\n",
    "\\nabla_{z^{(l)}}L(f(x),y)\n",
    "$$\n",
    "We can apply them throughout all layers, starting from $\\nabla_{z^{(2)}}L(f(x), y) = z^{(2)} - y$\n",
    "\n",
    "The reported loss graph is the following:\n",
    "![Neural Network Loss](results/q2/2_b/loss_lr1e-3_150ep.png)\n",
    "In the initial epochs, the loss values were high, but quickly lowered toward 0.2. In the following epochs, the most significant changes happened in the form of spikes, most likely due to steps in the wrong direction in gradient descent.\n",
    "Compared to the linear model, there was a smaller gap between the training and test sets, hinting that in the neural network overfitting did not occur. Furthermore, we obtained a smaller loss at the final epoch in the neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kVIxnsZX1OyC",
   "metadata": {
    "id": "kVIxnsZX1OyC"
   },
   "source": [
    "---\n",
    "### Question 3 - **Image classification with linear classifiers and neural networks**.\n",
    "\n",
    "> In this exercise, you will implement a linear classifier for a simple image classification problem, using the FashionMNIST dataset. Examples of images in this dataset are shown in Figure 1.  \n",
    "**Please do not use any machine learning library such as `scikit-learn` or similar for this exercise; just plain linear algebra (the numpy library is fine).** Python skeleton code is provided (`hw1-q3.py`).  \n",
    "In order to complete this exercise, you will need to download the Fashion-MNIST dataset. You can do this by running the following command in the homework directory:\n",
    "> ```\n",
    "python download_fashion_mnist.py\n",
    "> ```\n",
    "\n",
    "\n",
    "#### 3.1.a (5 points)\n",
    "> 1. Implement the `update_weights` method of the `Perceptron` class in `hw1-q3.py`.\n",
    "2. Then train $20$ epochs of the perceptron on the training set and\n",
    "3. Report its performance on the validation and test set.\n",
    "4. Plot the accuracies as a function of the epoch number.\n",
    "You can do this with the command\n",
    "> ```\n",
    "python h1-q3.py perceptron\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dyc9pxm3Hxm",
   "metadata": {
    "id": "4dyc9pxm3Hxm"
   },
   "source": [
    "The graph below depicts the accuracy scores attained by the perceptron in each epoch, for the validation and test data sets.\n",
    "\n",
    "![20Epochs](results/q3/1_a/acc_20-epochs.png)\n",
    "\n",
    "We can see that the accuracy scores vary between 0.7 and 0.8 and that the perceptron has a similar performance for the validation and test sets, however with slightly higher scores for the former (the biggest difference is 0.0113). We believe the perceptron had a similar performance in both data sets since none of them contained data the perceptron trained with.\n",
    "\n",
    "We can also note that the accuracy score oscillates drastically in each epoch, reaching amplitudes around 10% in some epochs. This result may suggest that the perceptron already reached its maximum accuracy.\n",
    "For this reason, we decided to analyze the accuracy scores the perceptron gets in 100 and 1000 epochs. The obtained results are shown in the graphs below:\n",
    "\n",
    "![100Epochs](results/q3/1_a/acc_100-epochs.png)\n",
    "![1000Epochs](results/q3/1_a/acc_1000-epochs.png)\n",
    "\n",
    "The maximum accuracy obtained in these runs was 0.840, which corroborates the previous idea.\n",
    "\n",
    "We know that the perceptron is a Linear classifier and is unable to represent non-linearly separable problems. The limited accuracy results obtained in this experiment suggest that the classification problem of this exercise may be too complex to be represented in a single perceptron and that we would need a more complex model, such as a Neural Network to achieve better results. This explains why the perceptron is underfitting the training data and having such low accuracies.\n",
    "\n",
    "Finally, we verified that adding a learning rate to the perceptron did not change the accuracy results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ztYziiST3Jbb",
   "metadata": {
    "id": "ztYziiST3Jbb"
   },
   "source": [
    "---\n",
    "#### 3.1.b (5 points)\n",
    "> 1. Repeat the same exercise using logistic regression instead (without regularization), using stochastic gradient descent as your training algorithm. Set a fixed learning rate $\\eta = 0.001$. This can be solved by implementing the `update_weights` method in the `LogisticRegression` class.  \n",
    "You can do this with the command\n",
    "> ```\n",
    "python h1-q3.py logistic_regression\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vSdcYv7y3rLF",
   "metadata": {
    "id": "vSdcYv7y3rLF"
   },
   "source": [
    "After implementing and running the logistic regression model, we obtained the following results:\n",
    "\n",
    "![20Epochs](results/q3/1_b/acc_10-epochs_lr-1e-3.png)\n",
    "\n",
    "We can observe that this model achieves accuracy scores similar to the ones attained by the previous model, but with much less oscillation. \n",
    "\n",
    "We believe the smoother results stem from the gradient that is calculated in the logistic regression, which is given as:\n",
    "\n",
    "$$\n",
    "\\nabla = e_y \\phi(x)^T - \\sum_y P_w(y’ | x)e_{y’}\\phi(x)^T\n",
    "$$\n",
    "\n",
    "This gradient corresponds to a matrix where each column $y’$ equals $\\phi(x)$ multiplied by the probability the model assigned to the class $y’$, except for the gold class.\n",
    "Applying this gradient to the weight matrix tunes up the weights of the correct class and tunes down the weights of every incorrect class, according to the probability assigned to that class.\n",
    "This means that the higher the score the model gives to an incorrect class, the higher the gradient that will be applied to the weights of that same class.\n",
    "Contrarily, the perceptron algorithm only updates the weights of the gold and incorrectly predicted classes, leaving the other classes as they were.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jIsZs-cG3vuX",
   "metadata": {
    "id": "jIsZs-cG3vuX"
   },
   "source": [
    "---\n",
    "#### 3.2\n",
    "\n",
    "> Now, you will implement a multi-layer perceptron (a feed-forward neural network) again using as input the original feature representation (i.e. simple independent pixel values).\n",
    "\n",
    "#### 3.2.a (5 points)\n",
    "> 1. Justify briefly why multi-layer perceptrons with non-linear activations are more expressive than the simple perceptron implemented above, and what kind of limitations they overcome for this particular task.\n",
    "2. Is this still the case if the activation function of the multi-layer perceptron is linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vpr8dZY44K0t",
   "metadata": {
    "id": "vpr8dZY44K0t"
   },
   "source": [
    "\n",
    "\n",
    "Multi-layer perceptrons with non-linear activations are more expressive than a single perceptron because the non-linear activation functions allow to perform non-linear operations on the input data. The combination of these non-linear activations, across different layers, allows to create more complex models that represent more complex problems, hence the increased expressiveness.\n",
    "\n",
    "This statement is supported by the following theorems:\n",
    "\n",
    "**Theorem (Hornik et al. (1989))**: An NN with one hidden layer and a linear output can approximate arbitrarily well any continuous function, given enough hidden units.\n",
    "\n",
    "**Theorem (Montufar et al. (2014))**: The number of linear regions carved out by a deep neural network with D inputs, depth L, and K hidden units per layer with ReLU activations is:\n",
    "$$\n",
    "O\\left(\n",
    "  \\left(\n",
    "  \\begin{array}{ll}\n",
    "    K\n",
    "    \\\\\n",
    "    D\n",
    "  \\end{array}\n",
    "  \\right)^{D(L-1)} K^D\n",
    "\\right)\n",
    "$$\n",
    "This means that, for a fixed number of hidden units, the networks are exponentially more expressive ([Lecture 05 slides](https://fenix.tecnico.ulisboa.pt/downloadFile/563568428828553/lecture_05.pdf)).\n",
    "\n",
    "Note that if the activation functions of a multi-layer perceptron are linear, then it is equivalent to a single perceptron and wil not be able to model more complex problems. (With linear activation functions it is possible to represent the entire feedforward process into a single matrix).\n",
    "\n",
    "The results obtained in 3.1.a hint that the problem may be too complex to be represented by a single perceptron (which follows a linear model). A more complex model, such as a multi-layer perceptron with non-linear activation functions may be able to better model this problem and to consequently have higher accuracy scores for this classification task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u-yG5R8q4qw7",
   "metadata": {
    "id": "u-yG5R8q4qw7"
   },
   "source": [
    "---\n",
    "#### 3.2.b (10 points)\n",
    "> 1. **Without using any neural network toolkit**, implement a multi-layer perceptron with a single hidden layer to solve this problem, including the gradient backpropagation algorithm which is needed to train the model.\n",
    "2. Use 200 hidden units, a `relu` activation function for the hidden layers, and a multinomial logistic loss (also called cross-entropy) in the output layer. Don’t forget to include bias terms in your hidden units.\n",
    "3. Train the model with stochastic gradient descent with a learning rate of $0.001$.\n",
    "4. Initialize biases with zero vectors and values in weight matrices with $w_{ij} \\sim \\mathcal{N} (\\mu, \\sigma^{2})$ with $\\mu = 0.1$ and $\\sigma^{2} = 0.1^{2}$ (hint: use `numpy.random.normal`).  \n",
    "Run your code with the command\n",
    "> ```\n",
    "python hw1-q3.py mlp\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D0cXKiaH5iga",
   "metadata": {
    "id": "D0cXKiaH5iga"
   },
   "source": [
    "After implementing and executing the MLP, we got the accuracy results depicted below:\n",
    "\n",
    "![20Epochs](results/q3/2_b/acc_20-epochs.png)\n",
    "\n",
    "This shows that this model attained higher accuracy scores than the perceptron, as expected.\n",
    "\n",
    "Below we compare the accuracy of the perceptron, the logistic regression and the MLP:\n",
    "\n",
    "![Comparison](results/q3/2_b/q3_comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9jBy2T7X5mAO",
   "metadata": {
    "id": "9jBy2T7X5mAO"
   },
   "source": [
    "---\n",
    "### Question 4 - **Image classification with an autodiff toolkit**.\n",
    "\n",
    "> In the previous question, you had to write gradient backpropagation by hand.  \n",
    "This time, you will implement the same system using a deep learning framework with automatic differentiation. Pytorch skeleton code is provided (`hw1-q4.py`) but if you feel more comfortable with a different framework, you are free to use it instead.\n",
    "\n",
    "#### 4.1 (10 points)\n",
    "> 1. Implement a linear model with logistic regression, using stochastic gradient descent as your training algorithm (use a batch size of $1$).\n",
    "2. Train your model for $20$ epochs and tune the learning rate on your validation data, using the following values: $\\{0.001, 0.01, 0.1\\}$.\n",
    "3. For the best configuration, report it and plot two things: the training loss and the validation accuracy, both as a function of the epoch number.\n",
    "4. Report the final accuracy on the test set.\n",
    "5. In the skeleton code, you will need to implement the method `train_batch` and the class `LogisticRegression`'s `__init__()` and `forward()` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UnWaVlMA6T0B",
   "metadata": {
    "id": "UnWaVlMA6T0B"
   },
   "source": [
    "After tuning training the Logistic Regression model for 20 epochs, with learning rates of $\\{0.001, 0.01, 0.1\\}$, we concluded that the best learning rate for this problem is $0.001$.\n",
    "This learning rate granted the lowest loss values and the highest accuracy scores.\n",
    "\n",
    "Below, we present the accuracy and loss plots for this learning rate:\n",
    "![Plot 1](results/q4/1/acc.png)\n",
    "![Plot 1](results/q4/1/loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DgziqkES6YaM",
   "metadata": {
    "id": "DgziqkES6YaM"
   },
   "source": [
    "---\n",
    "#### 4.2 (10 points)\n",
    "> 1. Implement a feed-forward neural network with a single layer, using dropout regularization.\n",
    "2. Make sure to include all the hyperparameters and training/model design choices shown in Table 1. Use the values presented in the table as default.\n",
    "3. Tune each of these hyperparameters while leaving the remaining at their default value:\n",
    ">  * The learning rate: $\\{0.001, 0.01, 0.1\\}$.\n",
    ">  * The hidden size: $\\{100, 200\\}$.\n",
    ">  * The dropout probability: $\\{0.3, 0.5\\}$.\n",
    ">  * The activation function: `relu` and `tanh`.\n",
    ">  * The optimizer: `SGD` and `Adam`\n",
    "4. Report your best configuration, make similar plots as in the previous question, and report the final test accuracy.  \n",
    "> In the skeleton code, you will need to implement the class `FeedforwardNetwork`’s `__init__()` and `forward()` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PFczy06-7JG4",
   "metadata": {
    "id": "PFczy06-7JG4"
   },
   "source": [
    "For this experiment, we tuned the hyperparameters one by one, while leaving the others at their default value, and compared the different hyperparameter values using the attained accuracy in the validation data set.\n",
    "For each tuned hyperparameter, we generated a plot that helped comparing the results, and a table that frames the accuracy range and the final accuracy.\n",
    "\n",
    "We discuss our results hyperparameter by hyperparameter:\n",
    "\n",
    "**Learning Rate**\n",
    "\n",
    "![Learning Rate Tuning Plot](tuning/learning_rate.png)\n",
    "\n",
    "| Parameters            | Min. Acc. | Max. Acc. | Final Acc. |\n",
    "|-----------------------|-----------|-----------|------------|\n",
    "| Learning Rate = 0.1   | 0.101     | 0.203     | 0.203      |\n",
    "| Learning Rate = 0.01  | 0.802     | 0.875     | 0.868      |\n",
    "| Learning Rate = 0.001 | 0.777     | 0.867     | 0.867      |\n",
    "\n",
    "From the plot above, we can see that the configuration with Learning Rate of 0.1 (`LR=0.1`) reported accuracy scores much lower than the other two configurations.\n",
    "Moreover, it got a constant accuracy of 0.101 for every epoch except for the last one.\n",
    "We hypothesize that the low accuracy results are due to the learning rate being too high, which may make the model adjusting its parameters too much, in each SGD step.\n",
    "Hence, we decided to test the outlier configuration for 50 epochs, to analyse what happened to the accuracy.\n",
    "We observed that the accuracy varied between 0.101 and 0.231, reaching the final epoch with a value of 0.170 and a final test accuracy of 0.173.\n",
    "From this observation, we can conclude that **the learning rate of 0.1 is too high for this particular problem**, as lower learning rates present better results.\n",
    "We could not find any explanation for having constant accuracy during the first epochs, for this configuration.\n",
    "\n",
    "\n",
    "When it comes to pick the best Learning Rate, we can observe that the configuration with `LR=0.01` presented the highest minimum, maximum and final accuracy scores.\n",
    "Moreover, we can observe that, in the first epochs, `LR=0.001` got lower accuracy results than `LR=0.01`, which suggests that the lower learning rate is learning more slowly, which makes sense.\n",
    "For this reason, **we chose `LR=0.01` for the final configuration**.\n",
    "\n",
    "Nonetheless, we cannot say for sure that 0.01 is the best Learning Rate value for this problem, since both `LR=0.01` and `LR=0.001` configurations got very similar results.\n",
    "In fact, when experimenting with other configuration combinations, we got higher accuracy results with a lower `LR`:\n",
    "\n",
    "| Parameters           | Min. Acc. | Max. Acc. | Final Acc. |\n",
    "|----------------------|-----------|-----------|------------|\n",
    "| Learning Rate = 0.1  | 0.822     | 0.868     | 0.858      |\n",
    "| Learning Rate = 0.01 | 0.820     | 0.878     | 0.878      |\n",
    "\n",
    "\n",
    "\n",
    "**Hidden Size**\n",
    "\n",
    "![Learning Rate Tuning Plot](tuning/hidden_layer_size.png)\n",
    "\n",
    "\n",
    "| Parameters        | Min. Acc. | Max. Acc. | Final Acc. |\n",
    "|-------------------|-----------|-----------|------------|\n",
    "| Hidden Size = 100 | 0.822     | 0.868     | 0.858      |\n",
    "| Hidden Size = 200 | 0.802     | 0.875     | 0.868      |\n",
    "\n",
    "We could not identify a big difference in the performance these configurations, as the one that has the greater accuracy varies in different epochs.\n",
    "We opted to **choose the `HS=200` configuration as it yielded higher maximum and final accuracy.**\n",
    "\n",
    "Note that choosing a smaller Hidden Layer would also be a valid option since it would require smaller weight matrices and would need to perform less computations.\n",
    "\n",
    "\n",
    "**Dropout Probability**\n",
    "\n",
    "![Learning Rate Tuning Plot](tuning/dropout.png)\n",
    "\n",
    "\n",
    "| Parameters    | Min. Acc. | Max. Acc. | Final Acc. |\n",
    "|---------------|-----------|-----------|------------|\n",
    "| Dropout = 0.3 | 0.802     | 0.875     | 0.868      |\n",
    "| Dropout = 0.5 | 0.741     | 0.845     | 0.845      |\n",
    "\n",
    "We can observe that the `Dropout=0.3` configuration has higher accuracies than the `Dropout=0.5` configuration in every epoch, hence it is a good candidate for the best value for the Dropout.\n",
    "\n",
    "We cogitate that this may happen because a higher Dropout probabilities will deactivate more units during the feedforward, which may lead to a slower learning curve.\n",
    "\n",
    "After testing the configuration `Dropout=0.5` for 50 epochs, we observed that the maximum and final accuracy values were 0.856 and 0.848, respectively.\n",
    "This is still lower than the values got by the other configuration.\n",
    "\n",
    "\n",
    "**Activation Function**\n",
    "\n",
    "![Learning Rate Tuning Plot](tuning/activation_function.png)\n",
    "\n",
    "\n",
    "| Parameters          | Min. Acc. | Max. Acc. | Final Acc. |\n",
    "|---------------------|-----------|-----------|------------|\n",
    "| Activation = `relu` | 0.802     | 0.875     | 0.868      |\n",
    "| Activation = `tanh` | 0.795     | 0.839     | 0.837      |\n",
    "\n",
    "We can observe that, with the exception of epoch 3, the `Activation=relu` configuration got higher accuracy results than the `Activation=tanh` activation.\n",
    "Moreover, the gap between the accuracy of both configurations is larger than the one in other hyperparameters.\n",
    "This indicates that the `relu` Activation function might be the best suited for this problem.\n",
    "\n",
    "After researching about this topic, we found that\n",
    "> \"Due to vanishing gradient problem, \\[...\\] sigmoid and tanh functions are avoided; \\[...\\] The ReLU function is the most widely used function and performs better than other activation functions in most of the cases\"  \n",
    "> [Activation Functions in Neural Networks](https://www.ijeast.com/papers/310-316,Tesma412,IJEAST.pdf);\n",
    "\n",
    "and that\n",
    "> \"A property of the tanh function is that it can only attain a gradient of 1, only when the value of the input is 0, that is when x is zero. This makes the tanh function produce some dead neurons during computation. \\[...\\] This limitation of the tanh function spurred further research in activation functions to resolve the problem, and it birthed the rectified linear unit (ReLU) activation function.\"  \n",
    "> [Activation Functions: Comparison of Trends in Practice and Research for Deep Learning](https://arxiv.org/pdf/1811.03378.pdf);\n",
    "\n",
    "These conclusions suggest that the `ReLU` activation function is more likely to yield better accuracy scores, which is verified in the results of this experiment.\n",
    "\n",
    "**Optimizer**\n",
    "\n",
    "![Learning Rate Tuning Plot](tuning/optimizer.png)\n",
    "\n",
    "\n",
    "| Parameters         | Min. Acc. | Max. Acc. | Final Acc. |\n",
    "|--------------------|-----------|-----------|------------|\n",
    "| Optimizer = `Adam` | 0.101     | 0.101     | 0.101      |\n",
    "| Activation = `SGD` | 0.802     | 0.875     | 0.868      |\n",
    "\n",
    "The first interesting result is that the accuracy of the `Adam` optimizer is constant during every epoch and has much lower accuracy compared to `SGD`.\n",
    "This may hint that `Adam` may not be the best optimizer for this problem, however we cannot generalize this result for other scenarios.\n",
    "\n",
    "We further investigated this result and launched several tests, one for each hyperparameter combination that used the `Adam` optimizer, and got the following results:\n",
    "\n",
    "![Adam tuning](tuning/adam.png)\n",
    "\n",
    "This exercise showed that some configurations still had low accuracy scores, but 5 configurations managed to have accuracy scores above 0.500.\n",
    "The best registered configuration for the `Adam` optimizer had `LR=0.001`, `HS=100`, `Dropout=0.3`, `Activation=tanh`, `Optimizer=adam`, and `HL=1`. This configuration got accuracy values ranging between 0.746 and 0.814, with a final accuracy of 0.812.\n",
    "\n",
    "Interestingly enough, we could also notice that, from the top 10 configurations that use the `Adam` optimizer, only one of them used the `ReLU` activation function, bein ranked in 3rd place with accuracy values ranging between 0.718 and 0.771, with a final accuracy of 0.771.\n",
    "\n",
    "\n",
    "**\"Best\" Configuration**\n",
    "\n",
    "After analyzing all the hyperparameters, we hypothesize the best configuration is:\n",
    "\n",
    "| Parameter           | Value |\n",
    "|---------------------|-------|\n",
    "| Learning Rate       | 0.01  |\n",
    "| Hidden Size         | 200   |\n",
    "| Dropout             | 0.3   |\n",
    "| Activation Function | ReLu  |\n",
    "| Optimizer           | SGD   |\n",
    "\n",
    "After running this configuration we got the following result:\n",
    "\n",
    "![Best configuration accuracy](tuning/best.png)\n",
    "\n",
    "Since the best hyperparameters coincided always with the default parameters, the best configuration ended up being the default one.\n",
    "For this reason, the accuracy obtained in this experiment is the same as the accuracy we got in every other test, with the default value for the tuned hyperparameter.\n",
    "\n",
    "However, the extra experiments made when tuning the Learning Rate and the Optimizer suggested that we could have better accuracy scores for other hyperparamenter combinations.\n",
    "For this reason, we tested the accuracy for every combination and exported those values to a file (available in the [data directory](data)).\n",
    "We also developed a helper script ([graph.py](./graph.py)) to easily visualize this information.\n",
    "\n",
    "We concluded that the best configuration for this problem and for this number of epochs is:\n",
    "\n",
    "| Parameter           | \"Best\" | Actual Best |\n",
    "|---------------------|--------|-------------|\n",
    "| Learning Rate       | 0.01   | **0.001**   |\n",
    "| Hidden Size         | 200    | **100**     |\n",
    "| Dropout             | 0.3    | 0.3         |\n",
    "| Activation Function | ReLU   | ReLU        |\n",
    "| Optimizer           | SGD    | SGD         |\n",
    "\n",
    "We can compare the accuracies of both configurations in the following graph:\n",
    "\n",
    "![Best vs Actual Best](tuning/best_vs_actual_best.png)\n",
    "\n",
    "| Parameters  | Min. Acc. | Max. Acc. | Final Acc. |\n",
    "|-------------|-----------|-----------|------------|\n",
    "| \"Best\"      | 0.802     | 0.875     | 0.868      |\n",
    "| Actual Best | 0.820     | 0.878     | 0.878      |\n",
    "\n",
    "We can see that this configuration got an accuracy 1% higher in the \"Actual Best\" configuration.\n",
    "\n",
    "\n",
    "The fact that the best configuration does not correspond to the combination of the best hyperparameter individually may indicate that the hyperparameters influence each other.\n",
    "This hypothesis is supported by the results obtained when experimenting different configurations for the `Adam` optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vaRrdkn-7MGM",
   "metadata": {
    "id": "vaRrdkn-7MGM"
   },
   "source": [
    "---\n",
    "#### 4.3 (5 points)\n",
    "> 1. Using the same hyperparameters as in Table 1, increase the model to 2 and 3 layers.\n",
    "2. Report your best configuration, make similar plots as in the previous question, and report the final test accuracy.  \n",
    "(**Note**: in the real world, you would need to do hyperparameter tuning for the different network architectures, but this is not required for this assignment.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TNSL0GXa7Zor",
   "metadata": {
    "id": "TNSL0GXa7Zor"
   },
   "source": [
    "After executing the \"Best\" configuration with 1, 2 and 3 hidden layers, we got the following results:\n",
    "\n",
    "![\"Best\" configuration with 1, 2 and 3 layers](tuning/layers.png)\n",
    "\n",
    "| Parameters | Min. Acc. | Max. Acc. | Final Acc. |\n",
    "|------------|-----------|-----------|------------|\n",
    "| 1 Layer    | 0.802     | 0.875     | 0.868      |\n",
    "| 2 Layers   | 0.790     | 0.871     | 0.868      |\n",
    "| 3 Layers   | 0.758     | 0.870     | 0.863      |\n",
    "\n",
    "Although having the configuration with three layers took longer to learn, all of them finished the experiment with a similar accuracy score.\n",
    "Moreover, every configuration is the higher scorer in, at least, one epoch.\n",
    "\n",
    "If we were to choose the best, **we would pick the configuration with one layer for three reasons**:\n",
    "First, its accuracy scores varied between the larger of the minimum and the larger of the maximum;\n",
    "Second, having fewer layers leads to fewer computations and possibly faster results; and\n",
    "Third, it is the top scorer in more than 50% of the epochs.\n",
    "\n",
    "\n",
    "The results gotten in the previous question made us look for the best configuration considering different network depths.\n",
    "We called it `Actual Best All Layers` and below we can see its configuration:\n",
    "\n",
    "| Parameter           | \"Best\" | Actual Best | Actual Best All Layers |\n",
    "|---------------------|--------|-------------|------------------------|\n",
    "| Learning Rate       | 0.01   | **0.001**   | **0.001**              |\n",
    "| Hidden Size         | 200    | **100**     | 200                    |\n",
    "| Dropout             | 0.3    | 0.3         | 0.3                    |\n",
    "| Activation Function | ReLU   | ReLU        | ReLU                   |\n",
    "| Optimizer           | SGD    | SGD         | SGD                    |\n",
    "| Hidden Layers       | 1      | 1           | **2**                  |\n",
    "\n",
    "We got the following results:\n",
    "\n",
    "![\"Best\" configuration with 1, 2 and 3 layers](tuning/all_bests.png)\n",
    "\n",
    "| Parameters             | Min. Acc. | Max. Acc. | Final Acc. |\n",
    "|------------------------|-----------|-----------|------------|\n",
    "| \"Best\"                 | 0.802     | 0.875     | 0.868      |\n",
    "| Actual Best            | 0.820     | 0.878     | 0.878      |\n",
    "| Actual Best All Layers | 0.811     | 0.884     | 0.884      |\n",
    "\n",
    "We can see that this last configuration has higher accuracy scores in almost every epoch."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-apre] *",
   "language": "python",
   "name": "conda-env-.conda-apre-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
